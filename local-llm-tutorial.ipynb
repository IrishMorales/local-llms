{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c40e05-de4d-42a9-abac-07d84cf1aed9",
   "metadata": {},
   "source": [
    "# How to run an LLM locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bb48c-5fbe-4900-baad-1aeeb7cf6b23",
   "metadata": {},
   "source": [
    "Running an LLM locally can be done either through code or through an application/platform such as oobabooga, LM Studio, etc. This can also be done with either your own LLM architecture or by downloading a pre-trained LLM. The instructions below run a pre-trained LLM through code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc32e5-a7a6-4e20-b0cb-83006b2ed9f1",
   "metadata": {},
   "source": [
    "### Download Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c25b7-277d-4d6b-aed2-ef32d39b1f42",
   "metadata": {},
   "source": [
    "Each prerequisite can be downloaded from the linked site:\n",
    "- [Jupyter Notebook](https://jupyter.org/install)\n",
    "- [Python 3.7 or later](https://www.python.org/downloads/)\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "- [pip](https://pypi.org/project/pip/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a10d2c-b60b-4848-89f1-26a419f5a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.43.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\filelock-3.13.1-py3.11.egg (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\fsspec-2023.10.0-py3.11.egg (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\filelock-3.13.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\fsspec-2023.10.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\mpmath-1.3.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pygcn-0.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\irish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\sympy-1.12-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: C:\\Users\\Irish\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Download prerequisites\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03ecb2-829a-4265-a585-60855e4e770f",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6955e695-518e-4943-bcf0-bafd03f8e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317337ac-3c41-40e9-a58f-83f5d05a7ec0",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2cb0c5-13f3-4f7b-a2f5-96f2dd69b18b",
   "metadata": {},
   "source": [
    "This tutorial runs GPT-2 locally. If you would like to run a different LLM, select a model from the [Hugging Face Transformers library documentation](https://huggingface.co/docs/transformers/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14173caa-24f7-493a-896b-28293531fc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dcdd2fb21f4f4d80e9816be9c2fac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  77%|#######6  | 419M/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Irish\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19cf59d6cf6471f9ed7ecf3c90831ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained LLM and tokenizer\n",
    "model_name = \"gpt2\" # Replace with name of the LLM you want to run\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7988e9b-ec1c-4360-bc5f-09a267177a5c",
   "metadata": {},
   "source": [
    "### Choose device for model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08be24-4322-44ee-b132-46bf8c051d2e",
   "metadata": {},
   "source": [
    "The following commands define whether your model will be run on CPU or on GPU (sometimes referred to as CUDA). Note that the installation process for using PyTorch with CPU is different from using PyTorch with GPU. If any errors arise in the code below, ensure you have the correct PyTorch installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954934ec-7a9b-49cc-ad2a-47f9b164e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device to run the model (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039a602-2a64-4908-b12b-a07f25fb5058",
   "metadata": {},
   "source": [
    "### Feed input to model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bfb81f-0454-457f-9877-2d23f6c4708c",
   "metadata": {},
   "source": [
    "Create the input for your LLM and tokenize the input – this splits the input into “tokens” which the model understand. The tokens will then be placed on your selected device (either CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eddefec6-0881-41ee-8a15-edcc0af2d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Once upon a time\"\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0e661-4562-4eee-9a14-398a79b12056",
   "metadata": {},
   "source": [
    "Feed the input tokens into your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "233582a7-0f74-4586-8612-2119299e6e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "C:\\Users\\Irish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # Tells PyTorch not to train the model on your input\n",
    "    output_tokens = model.generate(input_tokens [\"input_ids\"], num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a000dbf-cf1f-44e1-a9f3-9a1c402eaf83",
   "metadata": {},
   "source": [
    "### Get model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f79a26-f9ed-4460-907b-4c80b291868d",
   "metadata": {},
   "source": [
    "Decode the generated output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bfa764f-2bfe-4d5b-a057-99dd87be84e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output_tokens [0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
